{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial with examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmfa.fluxomics_data.reaction import Reaction\n",
    "from cmfa.fluxomics_data.reaction_network import ReactionNetwork\n",
    "from cmfa.data_preparation import import_fluxomics_dataset_from_json\n",
    "from cmfa.fluxomics_data.compound import AtomPattern\n",
    "from cmfa.fluxomics_data.emu import EMU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/te/Library/CloudStorage/OneDrive-Personal/Biosustain/cmfa/cmfa/../data/test_data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "CUR_DIR = os.getcwd()\n",
    "TEST_DIR = f'{CUR_DIR}/../data/test_data'\n",
    "\n",
    "TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = import_fluxomics_dataset_from_json(f'{TEST_DIR}/model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Reaction id=R1, name=R1,stoichiometry={'A': {abc: -1.0}, 'B': {abc: 1.0}},reversible=False, ,\n",
       " Reaction id=R2, name=R2,stoichiometry={'B': {abc: -1.0}, 'D': {abc: 1.0}},reversible=True, ,\n",
       " Reaction id=R3, name=R3,stoichiometry={'B': {abc: -1.0}, 'C': {bc: 1.0}, 'E': {a: 1.0}},reversible=False, ,\n",
       " Reaction id=R4, name=R4,stoichiometry={'B': {abc: -1.0}, 'C': {de: -1.0}, 'D': {bcd: 1.0}, 'E': {a: 1.0, e: 1.0}},reversible=False, ,\n",
       " Reaction id=R5, name=R5,stoichiometry={'D': {abc: -1.0}, 'F': {abc: 1.0}},reversible=False, }"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reaction_network.reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound           A       B     C           D               E           F\n",
      "pattern          abc     abc    bc  de     abc   bcd         a     e   abc\n",
      "compound pattern                                                          \n",
      "A        abc      []    [R1]    []  []      []    []        []    []    []\n",
      "B        abc      []      []  [R3]  []  [R2_f]  [R4]  [R4, R3]    []    []\n",
      "C        bc       []      []    []  []      []    []        []    []    []\n",
      "         de       []      []    []  []      []  [R4]        []  [R4]    []\n",
      "D        abc      []  [R2_r]    []  []      []    []        []    []  [R5]\n",
      "         bcd      []      []    []  []      []    []        []    []    []\n",
      "E        a        []      []    []  []      []    []        []    []    []\n",
      "         e        []      []    []  []      []    []        []    []    []\n",
      "F        abc      []      []    []  []      []    []        []    []    []\n"
     ]
    }
   ],
   "source": [
    "print(model.reaction_network.reaction_adjacency_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: ('D', 'abc'), Value: ['R5']\n"
     ]
    }
   ],
   "source": [
    "df = model.reaction_network.reaction_adjacency_matrix()\n",
    "column_data = df[('F', 'abc')]\n",
    "\n",
    "# Step 2: Filter for non-empty cells\n",
    "# Assuming non-empty cells contain lists with at least one element, and empty cells contain empty lists.\n",
    "non_empty_cells = column_data[column_data.apply(lambda x: len(x)>0)]\n",
    "\n",
    "# Step 3: Retrieve the non-empty values and their corresponding rows\n",
    "for index, value in non_empty_cells.items():\n",
    "    print(f\"Row: {index}, Value: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import deque\n",
    "\n",
    "def _extract_atom_pattern(compound_pattern: str):\n",
    "    \"\"\"Extract the atom pattern from a compound pattern string.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    compound_pattern : str\n",
    "        The compound pattern string, e.g., \"Glu(abcde)\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The extracted atom pattern, e.g., \"abcde\".\n",
    "    \"\"\"\n",
    "    match = re.search(r'(.+)\\((.+)\\)', compound_pattern)\n",
    "    if match:\n",
    "        compound, pattern = match.groups()\n",
    "        return compound, pattern \n",
    "    else:\n",
    "        raise ValueError(f\"No atom pattern found in the input: {compound}\")\n",
    "\n",
    "def decompose_network(\n",
    "    target_compound: str, reaction_network: ReactionNetwork\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "\n",
    "    MAM = reaction_network.reaction_adjacency_matrix()\n",
    "    all_cpd = MAM.columns.get_level_values(0).unique()\n",
    "\n",
    "    queue = deque([target_compound])\n",
    "    visited = set()\n",
    "    emu_maps = {}\n",
    "\n",
    "    while queue:\n",
    "        # Keep poping new EMU added from previous round.\n",
    "        cur_emu = queue.popleft()\n",
    "        print(f\"Current EMU: {cur_emu}, have visited {visited}\")\n",
    "        if cur_emu in visited:\n",
    "            continue\n",
    "        visited.add(cur_emu)\n",
    "\n",
    "        # Pattern matching\n",
    "        _cur_emu = EMU(id=\"_\".join(cur_emu), compound=cur_emu[0], pattern=AtomPattern(pattern=cur_emu[1]))\n",
    "\n",
    "        emu_size = _cur_emu.size\n",
    "        if emu_size not in emu_maps:\n",
    "            emu_maps[emu_size] = {}\n",
    "        \n",
    "        # Find the reaction that is participated in the current emu\n",
    "        col = MAM[cur_emu]\n",
    "        reactants = col[col.apply(lambda x: len(x)>0)]\n",
    "\n",
    "        # Finding multiple reactants\n",
    "        _reactants_str = reactants.apply(lambda x: str(x))\n",
    "        multi_reactants = _reactants_str.duplicated(keep=False)\n",
    "\n",
    "        if multi_reactants.any():\n",
    "            # Use the duplicates mask to filter the original Series and get the multi-index keys\n",
    "            duplicate_keys = reactants[multi_reactants].index.tolist()\n",
    "\n",
    "            overlap_pat = find_matchable_parts(duplicate_keys, cur_emu[1])\n",
    "\n",
    "            for new_emu in overlap_pat:\n",
    "                queue.append(new_emu)\n",
    "        else:\n",
    "            # Add reactants to the queue and emu map\n",
    "            print(reactants, \"\\n\", )\n",
    "            for i in reactants.index:\n",
    "                print(i)\n",
    "                match_cpd_pat = [key for key in MAM.index if key[0] == i[0]]\n",
    "                for m in match_cpd_pat:\n",
    "                    queue.append(tuple(m))\n",
    "                # Add a condition to stop when the number of atoms are not matching -> Stop when this happens(B23 + C1)\n",
    "            \n",
    "        for next_emu in reactants:\n",
    "            emu_maps[emu_size].setdefault(cur_emu, []).append(next_emu)\n",
    "\n",
    "            \n",
    "    return emu_maps\n",
    "\n",
    "\n",
    "def find_matchable_parts(keys, match):\n",
    "    new_keys = []\n",
    "    for key in keys:\n",
    "        # Unpack the tuple\n",
    "        letter, sequence = key\n",
    "        \n",
    "        # Check for matchable parts and keep characters that are found in the matchable part\n",
    "        matched_sequence = ''.join([char for char in sequence if char in match])\n",
    "        \n",
    "        # Construct a new tuple with the matched parts\n",
    "        new_key = (letter, matched_sequence)\n",
    "        new_keys.append(new_key)\n",
    "        \n",
    "    return new_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current EMU: ('F', 'abc'), have visited set()\n",
      "compound  pattern\n",
      "D         abc        [R5]\n",
      "Name: (F, abc), dtype: object \n",
      "\n",
      "('D', 'abc')\n",
      "Current EMU: ('D', 'abc'), have visited {('F', 'abc')}\n",
      "compound  pattern\n",
      "B         abc        [R2_f]\n",
      "Name: (D, abc), dtype: object \n",
      "\n",
      "('B', 'abc')\n",
      "Current EMU: ('D', 'bcd'), have visited {('D', 'abc'), ('F', 'abc')}\n",
      "Current EMU: ('B', 'abc'), have visited {('D', 'abc'), ('D', 'bcd'), ('F', 'abc')}\n",
      "compound  pattern\n",
      "A         abc          [R1]\n",
      "D         abc        [R2_r]\n",
      "Name: (B, abc), dtype: object \n",
      "\n",
      "('A', 'abc')\n",
      "('D', 'abc')\n",
      "Current EMU: ('B', 'bc'), have visited {('D', 'abc'), ('D', 'bcd'), ('F', 'abc'), ('B', 'abc')}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('B', 'bc')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Biosustain/cmfa/.venv/lib/python3.12/site-packages/pandas/core/indexes/multi.py:3053\u001b[0m, in \u001b[0;36mMultiIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:771\u001b[0m, in \u001b[0;36mpandas._libs.index.BaseMultiIndexCodesEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2152\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.UInt64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2176\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.UInt64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 28",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecompose_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mabc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreaction_network\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 53\u001b[0m, in \u001b[0;36mdecompose_network\u001b[0;34m(target_compound, reaction_network)\u001b[0m\n\u001b[1;32m     50\u001b[0m     emu_maps[emu_size] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Find the reaction that is participated in the current emu\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m col \u001b[38;5;241m=\u001b[39m \u001b[43mMAM\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcur_emu\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m reactants \u001b[38;5;241m=\u001b[39m col[col\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m)]\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Finding multiple reactants\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Biosustain/cmfa/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4089\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 4089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4090\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Biosustain/cmfa/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4147\u001b[0m, in \u001b[0;36mDataFrame._getitem_multilevel\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   4146\u001b[0m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[0;32m-> 4147\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m   4149\u001b[0m         new_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/Biosustain/cmfa/.venv/lib/python3.12/site-packages/pandas/core/indexes/multi.py:3055\u001b[0m, in \u001b[0;36mMultiIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3053\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3057\u001b[0m     \u001b[38;5;66;03m# e.g. test_partial_slicing_with_multiindex partial string slicing\u001b[39;00m\n\u001b[1;32m   3058\u001b[0m     loc, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loc_level(key, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlevels)))\n",
      "\u001b[0;31mKeyError\u001b[0m: ('B', 'bc')"
     ]
    }
   ],
   "source": [
    "decompose_network(('F','abc'), model.reaction_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C', 'bc'), ('C', 'de')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAM = model.reaction_network.reaction_adjacency_matrix()\n",
    "all_cpd = MAM.columns.get_level_values(0).unique()\n",
    "selected_cpd_pat = [key for key in MAM.index if key[0] == 'C' ]\n",
    "selected_cpd_pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method from freeflux\n",
    "import numpy as np\n",
    "def decompose_network(ini_emus, lump = True, n_jobs = 1):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    ini_emus: dict\n",
    "        Metabolite ID => atom NOs or list of atom NOs. Atom NOs can be int list or str, \n",
    "        e.g., {'Ala': [[1,2,3], '23'], 'Ser': '123'}\n",
    "    lump: bool\n",
    "        Whether to lump linear EMUs.\n",
    "    n_jobs: int\n",
    "        # of jobs to run in parallel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mergedEAMs: dict of df\n",
    "        Size => merged EMU adjacency matrix (EAM)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    EMUs in sequential reactions can not be lumped in transient MFA.      \n",
    "    '''\n",
    "    \n",
    "    metabids = []\n",
    "    atom_nos = []\n",
    "    for metabid, atomNOs in ini_emus.items():\n",
    "        if isinstance(atomNOs, list):\n",
    "            if any(isinstance(item, Iterable) for item in atomNOs):\n",
    "                for atomnos in atomNOs:\n",
    "                    if not isinstance(atomnos, str):\n",
    "                        atomnos = ''.join(map(str, atomnos))\n",
    "                    metabids.append(metabid)\n",
    "                    atom_nos.append(atomnos)\n",
    "            else:\n",
    "                atomNOs = ''.join(map(str, atomNOs))\n",
    "                metabids.append(metabid)\n",
    "                atom_nos.append(atomnos)\n",
    "        else:\n",
    "            metabids.append(metabid)\n",
    "            atom_nos.append(atomNOs)            \n",
    "    \n",
    "    return _decompose_network(metabids, atom_nos, lump = lump, n_jobs = n_jobs) \n",
    "\n",
    "\n",
    "def _decompose_network(metabolites, atom_nos, lump = True):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    metabolites: list of str\n",
    "        List of metabolite IDs from which initial EMU will be generated to start the \n",
    "        decomposition.\n",
    "    atom_nos: list of str\n",
    "        Atom NOs of corresponding metabolites, len(atom_nos) should be equal to \n",
    "        len(metabolites).\n",
    "    lump: bool\n",
    "        Whether to lump linear EMUs.    \n",
    "    n_jobs: int\n",
    "        # of jobs to run in parallel.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mergedEAMs: dict of df\n",
    "        Size => merged EMU adjacency matrix (EAM).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    EMUs in sequential reactions can not be lumped in transient MFA.    \n",
    "    '''\n",
    "    \n",
    "    emus = []\n",
    "    for metabid, atomNOs in zip(metabolites, atom_nos):\n",
    "        emus.append(EMU(metabid+'_'+atomNOs, Metabolite(metabid), atomNOs))\n",
    "        \n",
    "    EAMsAll = []\n",
    "    for emu in emus:\n",
    "        EAMs = get_emu_adjacency_matrices(emu, lump)\n",
    "        EAMsAll.append(EAMs)\n",
    "    \n",
    "    mergedEAMs = _merge_all_EAMs(*EAMsAll)\n",
    "    \n",
    "    return mergedEAMs\n",
    "\n",
    "def _merge_all_EAMs(self, *EAMsAll):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    EAMsAll: tuple of EAMs\n",
    "        EAMs is dict of DataFrame, i.e., Size => EMU adjacency matrix (EAM).\n",
    "\n",
    "    Returns\n",
    "    -------    \n",
    "    mergedEAMs: dict of df\n",
    "        Size => merged EAM     \n",
    "    '''\n",
    "    \n",
    "    mergedEAMs = {}\n",
    "    maxsize = max([max(EAMs) for EAMs in EAMsAll])\n",
    "    for size in range(1, maxsize+1):\n",
    "        EAMCurrentSize = list(\n",
    "            filter(\n",
    "                lambda x: isinstance(x, pd.DataFrame), \n",
    "                [EAMs.get(size, 0) for EAMs in EAMsAll]\n",
    "            )\n",
    "        )\n",
    "        if EAMCurrentSize:   \n",
    "            mergedEAMs[size] = reduce(self._merge_EAMs, EAMCurrentSize)\n",
    "    \n",
    "    return mergedEAMs            \n",
    "\n",
    "    \n",
    "\n",
    "def get_emu_adjacency_matrices(self, iniEMU, lump = True):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        iniEMU: EMU\n",
    "            Starting EMU of the decomposition.\n",
    "        lump: bool\n",
    "            Whether to lump linear EMUs.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        EAMs: dict of df\n",
    "            Size => EMU adjacency matrix (EAM) after lumping of linear EMUs and combination\n",
    "            of equivalent EMUs. Index and columns are EMUs, cells are symbolic expression of flux.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        EMUs in sequential reactions can not be lumped in transient MFA.\n",
    "        '''\n",
    "            \n",
    "        oriEAMs = _get_original_EAMs(iniEMU)\n",
    "        \n",
    "        if lump:\n",
    "            lumpedEAMs = _lump_linear_EMUs(oriEAMs, iniEMU)\n",
    "            combinedEAMs = _combine_equivalent_EMUs(lumpedEAMs)\n",
    "        else:\n",
    "            combinedEAMs = _combine_equivalent_EMUs(oriEAMs)\n",
    "        \n",
    "        return combinedEAMs\n",
    "\n",
    "def _get_original_EAMs(self, iniEMU):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        iniEMU: EMU\n",
    "            Starting EMU of the decomposition.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        EAMs: dict of df\n",
    "            Size => original EMU adjacency matrix (EAM), cells are symbolic expression of fluxã€‚\n",
    "        '''\n",
    "        \n",
    "        EAMsInfo = _BFS(iniEMU)\n",
    "        \n",
    "        EAMs = {}\n",
    "        for size, EMUsInfo in EAMsInfo.items():\n",
    "            \n",
    "            nonSourceEMUs = set([EMUInfo[0] for EMUInfo in EMUsInfo])\n",
    "            SourceEMUs = sorted(\n",
    "                set(\n",
    "                    [tuple(EMUInfo[1]) if len(EMUInfo[1]) > 1 else EMUInfo[1][0] \n",
    "                     for EMUInfo in EMUsInfo]\n",
    "                ) - nonSourceEMUs\n",
    "            )\n",
    "            \n",
    "            EAM = pd.DataFrame(\n",
    "                Integer(0), \n",
    "                index = sorted(nonSourceEMUs) + sorted(SourceEMUs), \n",
    "                columns = sorted(nonSourceEMUs)\n",
    "            )\n",
    "            for emu, preEMUs, flux in EMUsInfo:\n",
    "                \n",
    "                col = emu\n",
    "                \n",
    "                if len(preEMUs) == 1:\n",
    "                    idx = preEMUs[0]\n",
    "                else:\n",
    "                    idx = tuple(preEMUs)\n",
    "                \n",
    "                EAM.loc[[idx], col] += flux   \n",
    "            \n",
    "            EAMs[size] = EAM\n",
    "        \n",
    "        return EAMs\n",
    "\n",
    "def _BFS(self, iniEMU):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        iniEMU: EMU\n",
    "            Starting EMU of the decomposition. \n",
    "            Metabolite of iniEMU can be any Metabolite instance with the same id.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        EAMsInfo: dict of list\n",
    "            Size => list of [EMU, [precursor EMUs], symbolic expression of flux].\n",
    "        '''\n",
    "        \n",
    "        MAM = self.metabolite_adjacency_matrix\n",
    "        \n",
    "        EAMsInfo = {}\n",
    "        \n",
    "        searched = []\n",
    "        toSearch = deque()\n",
    "        toSearch.appendleft(iniEMU)\n",
    "        while toSearch:\n",
    "            \n",
    "            currentEMU = toSearch.pop()\n",
    "            searched.append(currentEMU)\n",
    "                    \n",
    "            formingRxns = list(\n",
    "                set(chain(*[cell for cell in MAM[currentEMU.metabolite_id] if cell]))\n",
    "            )   \n",
    "            for formingRxn in formingRxns:\n",
    "                \n",
    "                if formingRxn.reversible:\n",
    "                    if currentEMU.metabolite_id in formingRxn.products_with_atoms:\n",
    "                        asProMetabs = formingRxn.products_info['metab'][currentEMU.metabolite_id]\n",
    "                        direction = 'forward'\n",
    "                        flux = formingRxn.fflux\n",
    "                    else:\n",
    "                        asProMetabs = formingRxn.substrates_info['metab'][currentEMU.metabolite_id]\n",
    "                        direction = 'backward'\n",
    "                        flux = formingRxn.bflux\n",
    "                else:\n",
    "                    asProMetabs = formingRxn.products_info['metab'][currentEMU.metabolite_id]\n",
    "                    direction = 'forward'\n",
    "                    flux = formingRxn.flux\n",
    "                \n",
    "                if isinstance(asProMetabs, pd.Series):\n",
    "                    offset = 1/asProMetabs.size\n",
    "                    asProMetabs = list(asProMetabs)\n",
    "                else:\n",
    "                    offset = 1.0\n",
    "                    asProMetabs = [asProMetabs]\n",
    "                \n",
    "                \n",
    "                for asProMetab in asProMetabs:    \n",
    "                    currentEMU = EMU(currentEMU.id, asProMetab, currentEMU.atom_nos)   \n",
    "                    preEMUsInfo = formingRxn._find_precursor_EMUs(currentEMU, direction = direction)\n",
    "                    \n",
    "                    for preEMUs, coe in preEMUsInfo:\n",
    "                        for preEMU in preEMUs:\n",
    "                            if preEMU not in searched and preEMU not in toSearch:\n",
    "                                toSearch.appendleft(preEMU)\n",
    "                            \n",
    "                        EAMsInfo.setdefault(currentEMU.size, []).append(\n",
    "                            [currentEMU, preEMUs, offset * coe * flux]\n",
    "                        )\n",
    "                            \n",
    "        return EAMsInfo\n",
    "\n",
    "def _find_precursor_EMUs(self, emu, direction = 'forward'):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    emu: EMU\n",
    "    direction: str\n",
    "        * For reversible reaction,\n",
    "        'forward' if emu is product and precursor emu(s) are substrates;\n",
    "        'backward' if emu is substrate and precursor emu(s) are products.\n",
    "        * For irreversible reaction, only 'forward' is acceptable.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    preEMUsInfo: list\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    For reaction like: A({'ab': 0.5, 'ba': 0.5}) + B({'c': 1}) -> C({'abc': 0.5, 'cba': 0.5}),\n",
    "    _find_precursor_EMUs(C12) returns\n",
    "    [[[A_12], 0.5],\n",
    "        [[B_1, A_2], 0.25],\n",
    "        [[B_1, A_1], 0.25]].\n",
    "    '''        \n",
    "    \n",
    "    if self.reversible:\n",
    "        if direction == 'forward':\n",
    "            atomMapping = self._substrates_atom_mapping\n",
    "            reacInfo = self.substrates_info['metab']\n",
    "        \n",
    "        elif direction == 'backward':\n",
    "            atomMapping = self._products_atom_mapping\n",
    "            reacInfo = self.products_info['metab']\n",
    "    \n",
    "    else:\n",
    "        if direction != 'forward':\n",
    "            raise ValueError('only \"forward\" is acceptable for irreversible reaction')\n",
    "        \n",
    "        atomMapping = self._substrates_atom_mapping\n",
    "        reacInfo = self.substrates_info['metab']\n",
    "    \n",
    "    \n",
    "    preEMUsInfoRaw = []\n",
    "    for scenario in atomMapping:\n",
    "        for atoms, coe in emu.metabolite.atoms_info.items():   \n",
    "            \n",
    "            preAtoms = {}\n",
    "            uniCoe = coe\n",
    "            for atom in [atoms[no-1] for no in emu.atom_nos]:\n",
    "                \n",
    "                pre, preAtomNO, preCoe = scenario[atom]   \n",
    "                \n",
    "                if pre not in preAtoms:\n",
    "                    uniCoe *= preCoe\n",
    "                    preAtoms[pre] = [preAtomNO]\n",
    "                else:\n",
    "                    preAtoms[pre].append(preAtomNO)\n",
    "            \n",
    "            preEMUs = [EMU(pre.id+'_'+''.join(map(str, sorted(preAtomNOs))), pre, preAtomNOs) \n",
    "                        for pre, preAtomNOs in preAtoms.items()]\n",
    "            preEMUsInfoRaw.append([preEMUs, uniCoe])\n",
    "    \n",
    "    preEMUsInfoRaw = [Counter({tuple(sorted(preEMUs)): coe}) \n",
    "                        for preEMUs, coe in preEMUsInfoRaw]   \n",
    "    preEMUsInfo = reduce(lambda x, y: x+y, preEMUsInfoRaw) \n",
    "    preEMUsInfo = [[list(preEMUs), coe] for preEMUs, coe in preEMUsInfo.items()]\n",
    "    \n",
    "    return preEMUsInfo\n",
    "    \n",
    "\n",
    "def _lump_linear_EMUs(self, EAMs, iniEMU):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    EAMs: dict of df\n",
    "        Size => original EMU adjacency matrix (EAM), cells are symbolic expression of flux.\n",
    "    iniEMU: EMU\n",
    "        Starting EMU of the decomposition.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lumpedEAMs: dict of df\n",
    "        Size => lumped EMU adjacency matrix (EAM), cells are symbolic expression of flux.\n",
    "    '''\n",
    "    \n",
    "    lumpedEAMs = {}\n",
    "    orderedSizes = sorted(EAMs.keys(), reverse = True)\n",
    "    for i, size in enumerate(orderedSizes):\n",
    "        \n",
    "        lumpedEAM = EAMs[size].copy(deep = 'all')\n",
    "        for emu in lumpedEAM.columns:\n",
    "            \n",
    "            preEMUs = lumpedEAM.index[lumpedEAM[emu] != 0]\n",
    "            if preEMUs.size == 1:   \n",
    "                preEMU = preEMUs[0]\n",
    "                \n",
    "                if (emu != iniEMU and \n",
    "                    (preEMU not in lumpedEAM.columns or lumpedEAM.loc[emu, preEMU] == 0)):\n",
    "                    \n",
    "                    lumpedEAM.drop(emu, axis = 1, inplace = True)\n",
    "                    \n",
    "                    lumpedEAM.index = self._replace_list_item(lumpedEAM.index, emu, preEMU)\n",
    "                    \n",
    "                    for j in range(i):   \n",
    "                        largerEAM = lumpedEAMs[orderedSizes[j]]\n",
    "                        largerEAM.index = self._replace_list_item(largerEAM.index, emu, preEMU)\n",
    "                    \n",
    "                    lumpedEAM = _uniquify_dataFrame_index(lumpedEAM)   \n",
    "                    upper = _uniquify_dataFrame_index(\n",
    "                        lumpedEAM.loc[lumpedEAM.columns, :]\n",
    "                    )\n",
    "                    lower = _uniquify_dataFrame_index(\n",
    "                        lumpedEAM.loc[lumpedEAM.index.difference(lumpedEAM.columns),:]\n",
    "                    )\n",
    "                    lumpedEAM = pd.concat((upper, lower))\n",
    "        \n",
    "        lumpedEAMs[size] = lumpedEAM\n",
    "        \n",
    "    return lumpedEAMs\n",
    "    \n",
    "    \n",
    "def _combine_equivalent_EMUs(self, EAMs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    EAMs: dict of df\n",
    "        Size => original EMU adjacency matrix (EAM), cells are symbolic expression of flux.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    combinedEAMs: dict df\n",
    "        Size => EAM with equivalent EMUs combined, cells are symbolic expression of flux.\n",
    "    '''\n",
    "    \n",
    "    combinedEAMs = {}\n",
    "    for size, EAM in EAMs.items():\n",
    "        \n",
    "        combinedEAM = EAM.copy(deep = 'all')\n",
    "        combined = []\n",
    "        for emu in combinedEAM.columns:\n",
    "            if emu not in combined:\n",
    "                \n",
    "                equivEMU = emu.equivalent\n",
    "                if equivEMU in combinedEAM.columns:   \n",
    "                    \n",
    "                    combinedEAM.loc[:, emu] = combinedEAM.loc[:, [emu, equivEMU]].sum(axis = 1)/2\n",
    "                    combinedEAM.drop(equivEMU, axis = 1, inplace = True)\n",
    "                    \n",
    "                    combinedEAM.loc[emu, :] = combinedEAM.loc[[emu, equivEMU], :].sum()\n",
    "                    combinedEAM.drop(equivEMU, inplace = True)\n",
    "                    \n",
    "                    combined.append(equivEMU)\n",
    "                \n",
    "        combinedEAMs[size] = combinedEAM\n",
    "\n",
    "    return combinedEAMs\n",
    "\n",
    "def _uniquify_dataFrame_index(self, df):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: df\n",
    "        DataFrame to be uniquify.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    uniqueDf: df\n",
    "        DataFrame with duplicate rows combined (summated).\n",
    "    '''\n",
    "    \n",
    "    sortedDf = df.sort_index()\n",
    "\n",
    "    sortDfIndex, idx = np.unique(sortedDf.index, return_index = True)\n",
    "\n",
    "    uniqueDf = pd.DataFrame(\n",
    "        np.add.reduceat(sortedDf.values, idx), \n",
    "        index = sortDfIndex, \n",
    "        columns = sortedDf.columns\n",
    "    )\n",
    "    \n",
    "    return uniqueDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define the EMU class.'''\n",
    "\n",
    "\n",
    "__author__ = 'Chao Wu'\n",
    "__date__ = '02/26/2022'\n",
    "\n",
    "\n",
    "from functools import lru_cache\n",
    "from collections.abc import Iterable\n",
    "from .metabolite import Metabolite\n",
    "\n",
    "\n",
    "class EMU():\n",
    "    '''\n",
    "    Define EMU (i.e., elementary metabolite unit) object and its operations.\n",
    "\n",
    "    EMUs in the same metabolite and with the same atom NOs are considered as identical, \n",
    "    while metabolites which they derived from could be different.\n",
    "    \n",
    "    EMUs can be compared based self.metabolite_id and self.atom_nos.\n",
    "    EMU and iterable object of EMUs can also be compared. In this case EMU will be put into\n",
    "    the same iterable object with single item, and comparison between two iterables are performed.\n",
    "    \n",
    "    Currently only binary equivalents are considered.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    id: str\n",
    "        EMU ID\n",
    "    metabolite: Metabolite or str\n",
    "        Which metabolite the EMU comes from.\n",
    "    atom_nos: list of int or str\n",
    "        Atom NOs, sorted by number.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    id: str\n",
    "        EMU ID\n",
    "    metabolite: Metabolite\n",
    "        Which metabolite the EMU comes from.    \n",
    "    metabolite_id: str\n",
    "        Metabolite ID.\n",
    "    atom_nos: list of int\n",
    "        Atom NOs, sorted by number.\n",
    "    size: int\n",
    "        Size of EMU.\n",
    "    equivalent_atom_nos: None or list of int\n",
    "        Equivalent atom NOs, sorted by number.   \n",
    "    equivalent: EMU\n",
    "        Equivalent of EMU.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, id, metabolite, atom_nos):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        id: str\n",
    "            EMU ID\n",
    "        metabolite: Metabolite or str\n",
    "            Which metabolite the EMU comes from.\n",
    "        atom_nos: list of int or str\n",
    "            Atom NOs, sorted by number.\n",
    "        '''\n",
    "        \n",
    "        self.id = id\n",
    "        if isinstance(metabolite, Metabolite):\n",
    "            self.metabolite = metabolite\n",
    "            self.metabolite_id = self.metabolite.id\n",
    "        elif isinstance(metabolite, str):\n",
    "            self.metabolite = Metabolite(metabolite)\n",
    "            self.metabolite_id = metabolite\n",
    "        if isinstance(atom_nos, list):\n",
    "            self.atom_nos = sorted(atom_nos)\n",
    "        elif isinstance(atom_nos, str):\n",
    "            self.atom_nos = sorted(list(map(int, atom_nos)))\n",
    "        self.size = len(self.atom_nos)\n",
    "        \n",
    "    \n",
    "    def __hash__(self):\n",
    "        \n",
    "        return hash(self.metabolite_id) + hash(sum(self.atom_nos))\n",
    "        \n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        other: EMU or iterable\n",
    "        '''\n",
    "        \n",
    "        if isinstance(other, Iterable):\n",
    "            return type(other)([self]) == other\n",
    "        else:\n",
    "            return self.metabolite_id == other.metabolite_id and self.atom_nos == other.atom_nos\n",
    "        \n",
    "        \n",
    "    def __lt__(self, other):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        other: EMU or iterable\n",
    "        '''\n",
    "        \n",
    "        if isinstance(other, Iterable):\n",
    "            return type(other)([self]) < other\n",
    "        else:\n",
    "            if self.metabolite_id != other.metabolite_id:\n",
    "                return self.metabolite_id < other.metabolite_id\n",
    "            else:\n",
    "                return self.atom_nos < other.atom_nos\n",
    "                \n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        other: EMU or iterable\n",
    "        '''\n",
    "        \n",
    "        if isinstance(other, Iterable):\n",
    "            return type(other)([self]) > other\n",
    "        else:\n",
    "            if self.metabolite_id != other.metabolite_id:\n",
    "                return self.metabolite_id > other.metabolite_id\n",
    "            else:\n",
    "                return self.atom_nos > other.atom_nos\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    @lru_cache()\n",
    "    def equivalent_atom_nos(self):\n",
    "        '''\n",
    "        Returns\n",
    "        -------\n",
    "        equivAtomNOs: list of int or None\n",
    "            Equivalent atom NOs, sorted by number.\n",
    "        '''\n",
    "        \n",
    "        if len(self.metabolite.atoms_info) == 1:\n",
    "            return None\n",
    "\n",
    "        else:\n",
    "            refAtoms, equivAtoms = self.metabolite.atoms_info   \n",
    "            \n",
    "            mapping = dict(zip(refAtoms, range(1, len(refAtoms)+1)))\n",
    "            \n",
    "            equivAtomNOs = sorted([mapping[equivAtoms[no-1]] for no in self.atom_nos])\n",
    "            \n",
    "            if equivAtomNOs == self.atom_nos:\n",
    "                return None\n",
    "            else:\n",
    "                return equivAtomNOs\n",
    "                \n",
    "                \n",
    "    @property\n",
    "    @lru_cache()\n",
    "    def equivalent(self):\n",
    "        '''\n",
    "        Returns\n",
    "        -------\n",
    "        EMU: EMU\n",
    "            Equivalent of current EMU.\n",
    "        '''\n",
    "        \n",
    "        equivAtomNOs = self.equivalent_atom_nos\n",
    "        \n",
    "        if equivAtomNOs:\n",
    "            id = self.metabolite_id + '_' + ''.join(map(str, self.equivalent_atom_nos))\n",
    "            metab = self.metabolite\n",
    "            return EMU(id, metab, equivAtomNOs)\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return f'{self.__class__.__name__} {self.metabolite_id}_{\"\".join(map(str, self.atom_nos))}'\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
